# 获取mooc中兴义民族师范学院中的课程信息，以及对应的链接

import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
import time

def getSource(url):  #获取页面信息，并保存在csv中
    browser = webdriver.Chrome()  # 自动化打开浏览器，模拟人为操作
    browser.get(url)  # 打开需要的网页
    for i in range(1, 6):
        if(i == 1):  #如果i==1，那么不需要点击第一个页面，打开的时候显示的就是第一张页面
            time.sleep(5)  #让程序睡眠5秒，让页面加载完
            list_1 = getCourseInfo(browser.page_source)  #获取返回的列表
            saveCourseInfo(list_1)  #保存在csv中
        else:
            id1 = '.zpgi.zpg' + str(i)  #提供跳转页面的id值
            browser.find_element_by_css_selector(id1).click()  #点击跳转
            time.sleep(5)
            list_1 = getCourseInfo(browser.page_source)
            saveCourseInfo(list_1)
    browser.quit()

def getCourseInfo(page):  #获取信息
    f = []
    soup = BeautifulSoup(page, 'html.parser')  # 将网页的源码用BeautifulSoup的html.parser转换
    find_div = soup.find('div', attrs={"class": "g-container"})
    find_div1 = find_div.find('div', attrs={"class": "um-spoc-course-list_wrap"})
    find_div2 = find_div1.find_all('div', attrs={"class": "u-courseCardWithTime-container"})
    for li in find_div2:
        f1 = []
        href = li.find('a', attrs={"class": "u-courseCardWithTime-container_a160"})
        text = href.find('div', attrs={"class": "u-courseCardWithTime-teacher"})
        text1 = text.find('span').string  #获取span里的文本值
        f1.append(text1)  #添加到列表中
        f1.append('http:'+href.get('href'))  #获取对应课程的链接，添加到列表中
        f.append(f1)  #添加到列表中，形成一个二维矩阵
    return f

def saveCourseInfo(lis):  #保存到csv的方法
    df = pd.DataFrame(lis)  #将获取的list转换成DataFrame
    df.to_csv('x.csv', index=False, header=False, mode='a', encoding='gbk')  #保存DataFrame中的内容，使用追加模式，不要行号，不要列号

if __name__ == '__main__':
    url = 'https://www.icourse163.org/spoc/university/GYD073'
    getSource(url)
